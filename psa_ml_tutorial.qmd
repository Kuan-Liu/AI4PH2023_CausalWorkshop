---
title: "Propensity Score Analysis with Machine Learning"
bibliography: book.bib
author: "Kuan Liu"
subtitle: "HAD 7002H Causal Inference Spring/Summer 2024"
format: 
  html: 
    code-block-bg: true
    self-contained: true
---

# Dataset - The Right Heart Catheterization

For this tutorial, we will be using the same right heart catheterization (RHC) dataset. 

## Data import and processing

```{r echo=TRUE, warning=FALSE, message=FALSE}
library(tidyverse)
options(scipen = 999)
data <- read.csv("data/rhc.csv", header=T)

# define exposure variable
data$A <- ifelse(data$swang1 =="No RHC", 0, 1)

# outcome is dth30, a binary outcome measuring survival status at day 30;
data$Y <- ifelse(data$dth30 =="No", 0, 1)
```

## Finalizing dataset for causal analysis

```{r echo=TRUE, warning=FALSE, message=FALSE, fig.align='center', fig.height=9}
# we create our analysis data by removing variables with large proportion of missing;
# and variables not used in the analysis;
data2 <- select(data, -c(cat2, adld3p, urin1, swang1,
                         sadmdte, dschdte, dthdte, lstctdte, death, dth30,
                         surv2md1, das2d3pc, t3d30, ptid)) 
data2 <- rename(data2, id = X)
```

#  Proposensity score analysis using machine learning technqiues

##  1 Super (Machine) Learning

-  Super learning can be used to obtain robust estimator. In a nut-shell it uses loss-function based ML tool and cross-validation to obtain the best prediction of model parameter of interest, based on a **weighted average of a library of machine learning algorithms**.

-  Guide to SuperLearner by Chris Kennedy at [https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html](https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html)

-  New visual guide created by [Katherine Hoffman](https://www.khstats.com/art/illustrations_viz.html)

```{r echo = FALSE, out.width="100%"}
knitr::include_graphics("image/superlearning.jpeg")
```

-  List of machine learning algorithms under SuperLearner R package

```{r echo=TRUE, warning=FALSE, message=FALSE}
library(SuperLearner)
listWrappers()
```

##  2 Using machine learning methods with PSA

-  We can use ML to model our propensity score model
-  The use of machine learning methods is more flexible than parametric methods (i.e., logistic regression)
    -  Not without a cost, usually the more flexible the methods are the more one is at risk of overfitting; Too much noise considered in the modelling often results in poor coverage probability.
    -  There is no one approach that out performs others, thus which approach to use should be evaluated case by case. 
    -  ML is generally suggested for large enough cohort and for modelling large set of covariates.
    -  It's always suggested to include results from conventional logistic regression approach as a sensitivity analysis in comparison of ML approaches.


-  Many approaches are included in the WeightIt package, [https://ngreifer.github.io/WeightIt/reference/method_super.html](https://ngreifer.github.io/WeightIt/reference/method_super.html)
    -  "gbm", Propensity score weighting using generalized boosted modeling (also known as gradient boosting machines)
    -  "super", Propensity score weighting using SuperLearner
    -  "bart", Propensity score weighting using Bayesian additive regression trees (BART)
        -  Bayesian Additive Regression Trees (BART) is a sum-of-trees model for approximating an unknown function. To avoid overfitting (of decision tree), BART uses a regularization prior that forces each tree to be able to explain only a limited subset of the relationships between the covariates and the predictor variable.


**Setting up data and PS model formula**
```{r echo=TRUE, warning=FALSE, message=FALSE}
require(tidyverse)
require(WeightIt)

covariates <- select(data2, -c(id, A, Y))
baselines <- colnames(covariates)
ps.formula <- as.formula(paste("A~", 
                paste(baselines, collapse = "+")))
```

### PS model with gradient boosting

-  computationally more demanding and it might take several minutes to run.

```{r echo=TRUE, eval=FALSE, warning=FALSE, message=FALSE}
IPTW_gbm <- weightit(ps.formula,
                 data = data2,
                 method = "gbm",
                 stabilize = TRUE)
# saving the model output as a R object to avoid rerunning the same model;
saveRDS(IPTW_gbm, file = "data/IPTW_gbm")
```

```{r echo=TRUE, warning=FALSE, message=FALSE}
# reading saved model output;
require(sjPlot)
IPTW_gbm <- readRDS(file = "data/IPTW_gbm")
summary(IPTW_gbm)

fit2_gbm <- glm(Y ~ A, 
            family = "binomial",
            weights = IPTW_gbm$weights,
            data = data2)
tab_model(fit2_gbm)
```

### PS model with Super Learner

```{r echo=TRUE, eval=FALSE, warning=FALSE, message=FALSE}
IPTW_SL <- weightit(ps.formula,
                 data = data2,
                 method = "super",
                 SL.library=c("SL.randomForest", "SL.glmnet", "SL.nnet"), 
                 stabilize = TRUE)
# saving the model output as a R object to avoid rerunning the same model;
saveRDS(IPTW_SL, file = "data/IPTW_SL")
```

```{r echo=TRUE, warning=FALSE, message=FALSE}
# reading saved model output;
IPTW_SL <- readRDS(file = "data/IPTW_SL")
summary(IPTW_SL)

fit2_SL <- glm(Y ~ A, 
            family = "binomial",
            weights = IPTW_SL$weights,
            data = data2)
tab_model(fit2_SL)
```


### PS model with Bayesian additive regression trees

-  A much faster algorithm comparing to gbm and SL.

```{r echo=TRUE, eval=FALSE, warning=FALSE, message=FALSE}
IPTW_bart <- weightit(ps.formula,
                 data = data2,
                 method = "bart",
                 stabilize = TRUE)
# saving the model output as a R object to avoid rerunning the same model;
saveRDS(IPTW_bart, file = "data/IPTW_bart")
```

```{r echo=TRUE, warning=FALSE, message=FALSE}
# reading saved model output;
IPTW_bart <- readRDS(file = "data/IPTW_bart")
summary(IPTW_bart)

fit2_bart <- glm(Y ~ A, 
            family = "binomial",
            weights = IPTW_bart$weights,
            data = data2)
tab_model(fit2_bart)
```

# TMLE in Steps

```{r echo=TRUE, eval=T, warning=FALSE, message=FALSE}
set.seed(123)
library(xgboost)
library(tmle)
data2.noY <- select(data2, -c(id, Y))
```

## 1. Initial G-formula estimate using superlearner

```{r echo=TRUE, eval=FALSE, warning=FALSE, message=FALSE}
Y.fit.sl <- SuperLearner(Y=data2$Y, 
                       X=data2.noY, 
                       cvControl = list(V = 3),
                       SL.library=c("SL.glm", 
                                    "SL.glmnet", 
                                    "SL.xgboost"),
                       method="method.CC_nloglik", 
                       family="binomial")
saveRDS(Y.fit.sl, file = "data/SL_TMLE_G1")
```

## 2. Getting predicted Y 

```{r echo=TRUE, warning=FALSE, message=FALSE}
# reading saved SL G-formula output;
Y.fit.sl <- readRDS(file = "data/SL_TMLE_G1")
# predict Y under the observed A;
data2$init.Pred <- predict(Y.fit.sl, newdata = data2.noY, 
                           type = "response")$pred
summary(data2$init.Pred)

#treated;
data2.noY$A <- 1
data2$Pred.Y1 <- predict(Y.fit.sl, newdata = data2.noY, 
                           type = "response")$pred
summary(data2$Pred.Y1)

#control;
data2.noY$A <- 0
data2$Pred.Y0 <- predict(Y.fit.sl, newdata = data2.noY, 
                           type = "response")$pred
summary(data2$Pred.Y0)

# Compute initial treatment effect estimate;
data2$Pred.TE <- data2$Pred.Y1 - data2$Pred.Y0   
summary(data2$Pred.TE) 
plot(density(data2$Pred.TE))
```


## 3. Fitting Treatment Model (PS model)

```{r echo=TRUE, eval = FALSE,warning=FALSE, message=FALSE}
set.seed(124)
#covariates are variables not including A & Y;
# covariates <- select(data2, -c(id, A, Y))
PS.fit.SL <- SuperLearner(Y=data2$A, 
                       X=covariates, 
                       cvControl = list(V = 3),
                       SL.library=c("SL.glm", 
                                    "SL.glmnet", 
                                    "SL.xgboost"),
                       method="method.CC_nloglik",
                       family="binomial")
saveRDS(PS.fit.SL, file = "data/SL_TMLE_PS")
```

### Predict propensity scores

```{r echo=TRUE, warning=FALSE, message=FALSE}
# reading saved SL PS output;
PS.fit.SL <- readRDS(file = "data/SL_TMLE_PS")

# predict A = 1 given covariates;
all.pred <- predict(PS.fit.SL, type = "response")
data2$PS.SL <- all.pred$pred 

tapply(data2$PS.SL, data2$A, summary)

plot(density(data2$PS.SL[data2$A==0]), 
     col = "red", main = "")
lines(density(data2$PS.SL[data2$A==1]), 
      col = "blue", lty = 2)
legend("topright", c("No RHC","RHC"), 
       col = c("red", "blue"), lty=1:2) 

```
## 4. Estimate clever covariate H

```{r echo=TRUE, warning=FALSE, message=FALSE}
data2$H.A1L <- (data2$A) / data2$PS.SL 
data2$H.A0L <- (1-data2$A) / (1- data2$PS.SL)
data2$H.AL <- data2$H.A1L - data2$H.A0L
summary(data2$H.AL)
tapply(data2$H.AL, data2$A, summary)
```

## 5. Estimate $e$ the fluctuation parameter

```{r echo=TRUE, warning=FALSE, message=FALSE}
eps_mod <- glm(Y ~ -1 + H.AL +  
                 offset(qlogis(init.Pred)), 
               family = "binomial",
               data = data2)
epsilon <- coef(eps_mod)  
epsilon
```
> $e$ is pretty small! We probably don't need to update $e$.

## 6. Update $e$

```{r echo=TRUE, warning=FALSE, message=FALSE}
data2$Pred.Y1.update1 <- plogis(qlogis(data2$Pred.Y1) +  
                                   epsilon*data2$H.AL)
data2$Pred.Y0.update1 <- plogis(qlogis(data2$Pred.Y0) + 
                                   epsilon*data2$H.AL)
summary(data2$Pred.Y1.update1)
summary(data2$Pred.Y0.update1) 

data2$update.Pred <- ifelse(data2$A==1, data2$Pred.Y1.update1, data2$Pred.Y0.update1)

eps_mod1 <- glm(Y ~ -1 + H.AL +  
                 offset(qlogis(update.Pred)), 
               family = "binomial",
               data = data2)
epsilon1 <- coef(eps_mod1)  
epsilon1

```

## 7. Estimate ATE

```{r echo=TRUE, warning=FALSE, message=FALSE}

data2$Pred.Y1.update2 <- plogis(qlogis(data2$Pred.Y1.update1) +  
                                   epsilon1*data2$H.AL)
data2$Pred.Y0.update2 <- plogis(qlogis(data2$Pred.Y0.update1) + 
                                   epsilon1*data2$H.AL)
summary(data2$Pred.Y1.update2)
summary(data2$Pred.Y0.update2) 

# Risk difference;
ATE <- data2$Pred.Y1.update2 -  data2$Pred.Y0.update2
summary(ATE)
ATE.TMLE<-mean(ATE)

```

## 8. Estimate confidence interval using influence curve

```{r echo=TRUE, warning=FALSE, message=FALSE}

ci.estimate <- function(data = data2){

  # transform predicted outcomes back to original scale
  EY1 <- mean(data$Pred.Y1.update2, na.rm = TRUE)
  EY0 <- mean(data$Pred.Y0.update2, na.rm = TRUE)
  # ATE efficient influence curve
  D1 <- data$A/data$PS.SL*
    (data$Y - data$Pred.Y1.update2) + 
    data$Pred.Y1.update2 - EY1
  D0 <- (1 - data$A)/(1 - data$PS.SL)*
    (data$Y - data$Pred.Y0.update2) + 
    data$Pred.Y0.update2 - EY0
  EIC <- D1 - D0
  # ATE variance
  n <- nrow(data)
  varHat.IC <- var(EIC, na.rm = TRUE)/n
  # ATE 95% CI
  ATE.TMLE.CI <- c(ATE.TMLE - 1.96*sqrt(varHat.IC), 
                   ATE.TMLE + 1.96*sqrt(varHat.IC))
  return(ATE.TMLE.CI) 
}

print(c(mean(ATE), ci.estimate(data = data2)))
```

# TMLE using tmle R package

```{r echo=TRUE, eval=FALSE, warning=FALSE, message=FALSE}
# run tmle from the tmle package 
SL.library = c("SL.glm", 
               "SL.glmnet", 
               "SL.xgboost")

tmle.fit <- tmle(Y = data2$Y, 
                   A = data2$A, 
                   W = covariates, 
                   family = "binomial",
                   V.Q = 3, #outcome model;
                   V.g = 3, #treatment model;
                   Q.SL.library = SL.library, 
                   g.SL.library = SL.library)

saveRDS(tmle.fit, file = "data/SL_TMLE")
```

### ATE estimated using tmle package

```{r echo=TRUE, warning=FALSE, message=FALSE}
# reading saved TMLE output;
tmle.fit <- readRDS(file = "data/SL_TMLE")
summary(tmle.fit)
```

