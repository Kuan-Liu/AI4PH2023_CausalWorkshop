[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tutorial on Causal Inference Using Machine Learning Methods",
    "section": "",
    "text": "Welcome to the causal inference machine learning tutorial!\nWorkshop materials in the github repository AI4PH2023_CausalWorkshop"
  },
  {
    "objectID": "index.html#data-import-and-processing",
    "href": "index.html#data-import-and-processing",
    "title": "Tutorial on Causal Inference Using Machine Learning Methods",
    "section": "Data import and processing",
    "text": "Data import and processing\n\nlibrary(tidyverse)\ndata &lt;- read.csv(\"data/rhc.csv\", header=T)\n\n# define exposure variable\ndata$A &lt;- ifelse(data$swang1 ==\"No RHC\", 0, 1)\n\n# outcome is dth30, a binary outcome measuring survival status at day 30;\ndata$Y &lt;- ifelse(data$dth30 ==\"No\", 0, 1)"
  },
  {
    "objectID": "index.html#data-visualization-on-missing-values",
    "href": "index.html#data-visualization-on-missing-values",
    "title": "Tutorial on Causal Inference Using Machine Learning Methods",
    "section": "Data visualization on missing values",
    "text": "Data visualization on missing values\n\nlibrary(naniar)\ngg_miss_var(data, facet=A, show_pct = TRUE)\n\n\n\n\n\n\n\n# try changing facet to dth30, this examines missingness by outcome;"
  },
  {
    "objectID": "index.html#finalizing-dataset-for-causal-analysis",
    "href": "index.html#finalizing-dataset-for-causal-analysis",
    "title": "Tutorial on Causal Inference Using Machine Learning Methods",
    "section": "Finalizing dataset for causal analysis",
    "text": "Finalizing dataset for causal analysis\n\n# we create our analysis data by removing variables with large proportion of missing;\n# and variables not used in the analysis;\ndata2 &lt;- select(data, -c(cat2, adld3p, urin1, swang1,\n                         sadmdte, dschdte, dthdte, lstctdte, death, dth30,\n                         surv2md1, das2d3pc, t3d30, ptid)) \ndata2 &lt;- rename(data2, id = X)\n\n# display data on Quarto page;\nlibrary(DT)\ndata2 %&gt;% datatable(\n  rownames = FALSE,\n  options = list(\n    columnDefs = list(list(className = 'dt-center', \n                      targets = 0:4))))\n\n\n\n\n\n# verify data structure;\nstr(data2)\n\n'data.frame':   5735 obs. of  51 variables:\n $ id      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ cat1    : chr  \"COPD\" \"MOSF w/Sepsis\" \"MOSF w/Malignancy\" \"ARF\" ...\n $ ca      : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ cardiohx: int  0 1 0 0 0 0 0 0 0 0 ...\n $ chfhx   : int  0 1 0 0 0 1 0 0 0 0 ...\n $ dementhx: int  0 0 0 0 0 0 0 0 0 0 ...\n $ psychhx : int  0 0 0 0 0 0 0 0 0 0 ...\n $ chrpulhx: int  1 0 0 0 0 1 0 0 0 0 ...\n $ renalhx : int  0 0 0 0 0 0 0 0 0 0 ...\n $ liverhx : int  0 0 0 0 0 0 0 0 0 0 ...\n $ gibledhx: int  0 0 0 0 0 0 0 0 0 0 ...\n $ malighx : int  1 0 1 0 0 0 1 0 0 1 ...\n $ immunhx : int  0 1 1 1 0 0 0 0 0 0 ...\n $ transhx : int  0 1 0 0 0 0 0 1 0 0 ...\n $ amihx   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ age     : num  70.3 78.2 46.1 75.3 67.9 ...\n $ sex     : chr  \"Male\" \"Female\" \"Female\" \"Female\" ...\n $ edu     : num  12 12 14.07 9 9.95 ...\n $ aps1    : int  46 50 82 48 72 38 29 25 47 48 ...\n $ scoma1  : int  0 0 0 0 41 0 26 100 0 0 ...\n $ meanbp1 : num  41 63 57 55 65 115 67 128 53 73 ...\n $ wblc1   : num  22.1 28.9 0.05 23.3 29.7 ...\n $ hrt1    : int  124 137 130 58 125 134 135 102 118 141 ...\n $ resp1   : num  10 38 40 26 27 36 10 34 30 40 ...\n $ temp1   : num  38.7 38.9 36.4 35.8 34.8 ...\n $ pafi1   : num  68 218 276 157 478 ...\n $ alb1    : num  3.5 2.6 3.5 3.5 3.5 ...\n $ hema1   : num  58 32.5 21.1 26.3 24 ...\n $ bili1   : num  1.01 0.7 1.01 0.4 1.01 ...\n $ crea1   : num  1.2 0.6 2.6 1.7 3.6 ...\n $ sod1    : int  145 137 146 117 126 138 136 136 136 146 ...\n $ pot1    : num  4 3.3 2.9 5.8 5.8 ...\n $ paco21  : num  40 34 16 30 17 68 45 26 40 30 ...\n $ ph1     : num  7.36 7.33 7.36 7.46 7.23 ...\n $ wtkilo1 : num  64.7 45.7 0 54.6 78.4 ...\n $ dnr1    : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ ninsclas: chr  \"Medicare\" \"Private & Medicare\" \"Private\" \"Private & Medicare\" ...\n $ resp    : chr  \"Yes\" \"No\" \"No\" \"Yes\" ...\n $ card    : chr  \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ neuro   : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ gastr   : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ renal   : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ meta    : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ hema    : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ seps    : chr  \"No\" \"Yes\" \"No\" \"No\" ...\n $ trauma  : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ ortho   : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ race    : chr  \"white\" \"white\" \"white\" \"white\" ...\n $ income  : chr  \"Under $11k\" \"Under $11k\" \"$25-$50k\" \"$11-$25k\" ...\n $ A       : num  0 1 1 0 1 0 0 0 0 1 ...\n $ Y       : num  0 0 0 0 1 0 0 0 0 0 ...\n\nsaveRDS(data2,file=\"data/data2\")"
  },
  {
    "objectID": "section1.html",
    "href": "section1.html",
    "title": "1. The implementation of standard causal inference aproaches in R",
    "section": "",
    "text": "Outlines\n\n\n\n\nDescriptive analysis\nPropensity score matching\nPropensity score weighting\nGeneral guideline on PSA analysis"
  },
  {
    "objectID": "section1.html#summary-statistics-of-baseline-variables-by-treatment-status",
    "href": "section1.html#summary-statistics-of-baseline-variables-by-treatment-status",
    "title": "1. The implementation of standard causal inference aproaches in R",
    "section": "0. Summary statistics of baseline variables by treatment status",
    "text": "0. Summary statistics of baseline variables by treatment status\n\nrequire(tidyverse)\nlibrary(tableone)\ndata2&lt;-readRDS(\"data/data2\")\ncovariates &lt;- select(data2, -c(id, A, Y))\nbaselines &lt;- colnames(covariates)\nbaselines\n\n [1] \"cat1\"     \"ca\"       \"cardiohx\" \"chfhx\"    \"dementhx\" \"psychhx\" \n [7] \"chrpulhx\" \"renalhx\"  \"liverhx\"  \"gibledhx\" \"malighx\"  \"immunhx\" \n[13] \"transhx\"  \"amihx\"    \"age\"      \"sex\"      \"edu\"      \"aps1\"    \n[19] \"scoma1\"   \"meanbp1\"  \"wblc1\"    \"hrt1\"     \"resp1\"    \"temp1\"   \n[25] \"pafi1\"    \"alb1\"     \"hema1\"    \"bili1\"    \"crea1\"    \"sod1\"    \n[31] \"pot1\"     \"paco21\"   \"ph1\"      \"wtkilo1\"  \"dnr1\"     \"ninsclas\"\n[37] \"resp\"     \"card\"     \"neuro\"    \"gastr\"    \"renal\"    \"meta\"    \n[43] \"hema\"     \"seps\"     \"trauma\"   \"ortho\"    \"race\"     \"income\"  \n\ntab0 &lt;- CreateTableOne(vars = baselines,\n                       data = data2, \n                       strata = \"A\", \n                       test = FALSE, #mute P-value calculation;\n                       smd = TRUE,\n                       addOverall = TRUE)\nprint(tab0, smd = TRUE, showAllLevels = FALSE)\n\n                        Stratified by A\n                         Overall         0               1               SMD   \n  n                        5735            3551            2184                \n  cat1 (%)                                                                0.583\n     ARF                   2490 (43.4)     1581 (44.5)      909 (41.6)         \n     CHF                    456 ( 8.0)      247 ( 7.0)      209 ( 9.6)         \n     Cirrhosis              224 ( 3.9)      175 ( 4.9)       49 ( 2.2)         \n     Colon Cancer             7 ( 0.1)        6 ( 0.2)        1 ( 0.0)         \n     Coma                   436 ( 7.6)      341 ( 9.6)       95 ( 4.3)         \n     COPD                   457 ( 8.0)      399 (11.2)       58 ( 2.7)         \n     Lung Cancer             39 ( 0.7)       34 ( 1.0)        5 ( 0.2)         \n     MOSF w/Malignancy      399 ( 7.0)      241 ( 6.8)      158 ( 7.2)         \n     MOSF w/Sepsis         1227 (21.4)      527 (14.8)      700 (32.1)         \n  ca (%)                                                                  0.107\n     Metastatic             384 ( 6.7)      261 ( 7.4)      123 ( 5.6)         \n     No                    4379 (76.4)     2652 (74.7)     1727 (79.1)         \n     Yes                    972 (16.9)      638 (18.0)      334 (15.3)         \n  cardiohx (mean (SD))     0.18 (0.38)     0.16 (0.37)     0.20 (0.40)    0.116\n  chfhx (mean (SD))        0.18 (0.38)     0.17 (0.37)     0.19 (0.40)    0.069\n  dementhx (mean (SD))     0.10 (0.30)     0.12 (0.32)     0.07 (0.25)    0.163\n  psychhx (mean (SD))      0.07 (0.25)     0.08 (0.27)     0.05 (0.21)    0.143\n  chrpulhx (mean (SD))     0.19 (0.39)     0.22 (0.41)     0.14 (0.35)    0.192\n  renalhx (mean (SD))      0.04 (0.21)     0.04 (0.20)     0.05 (0.21)    0.032\n  liverhx (mean (SD))      0.07 (0.26)     0.07 (0.26)     0.06 (0.24)    0.049\n  gibledhx (mean (SD))     0.03 (0.18)     0.04 (0.19)     0.02 (0.16)    0.070\n  malighx (mean (SD))      0.23 (0.42)     0.25 (0.43)     0.20 (0.40)    0.101\n  immunhx (mean (SD))      0.27 (0.44)     0.26 (0.44)     0.29 (0.45)    0.080\n  transhx (mean (SD))      0.12 (0.32)     0.09 (0.29)     0.15 (0.36)    0.170\n  amihx (mean (SD))        0.03 (0.18)     0.03 (0.17)     0.04 (0.20)    0.074\n  age (mean (SD))         61.38 (16.68)   61.76 (17.29)   60.75 (15.63)   0.061\n  sex = Male (%)           3192 (55.7)     1914 (53.9)     1278 (58.5)    0.093\n  edu (mean (SD))         11.68 (3.15)    11.57 (3.13)    11.86 (3.16)    0.091\n  aps1 (mean (SD))        54.67 (19.96)   50.93 (18.81)   60.74 (20.27)   0.501\n  scoma1 (mean (SD))      21.00 (30.27)   22.25 (31.37)   18.97 (28.26)   0.110\n  meanbp1 (mean (SD))     78.52 (38.05)   84.87 (38.87)   68.20 (34.24)   0.455\n  wblc1 (mean (SD))       15.65 (11.87)   15.26 (11.41)   16.27 (12.55)   0.084\n  hrt1 (mean (SD))       115.18 (41.24)  112.87 (40.94)  118.93 (41.47)   0.147\n  resp1 (mean (SD))       28.09 (14.08)   28.98 (13.95)   26.65 (14.17)   0.165\n  temp1 (mean (SD))       37.62 (1.77)    37.63 (1.74)    37.59 (1.83)    0.021\n  pafi1 (mean (SD))      222.27 (114.95) 240.63 (116.66) 192.43 (105.54)  0.433\n  alb1 (mean (SD))         3.09 (0.78)     3.16 (0.67)     2.98 (0.93)    0.230\n  hema1 (mean (SD))       31.87 (8.36)    32.70 (8.79)    30.51 (7.42)    0.269\n  bili1 (mean (SD))        2.27 (4.80)     2.00 (4.43)     2.71 (5.33)    0.145\n  crea1 (mean (SD))        2.13 (2.05)     1.92 (2.03)     2.47 (2.05)    0.270\n  sod1 (mean (SD))       136.77 (7.66)   137.04 (7.68)   136.33 (7.60)    0.092\n  pot1 (mean (SD))         4.07 (1.03)     4.08 (1.04)     4.05 (1.01)    0.027\n  paco21 (mean (SD))      38.75 (13.18)   39.95 (14.24)   36.79 (10.97)   0.249\n  ph1 (mean (SD))          7.39 (0.11)     7.39 (0.11)     7.38 (0.11)    0.120\n  wtkilo1 (mean (SD))     67.83 (29.06)   65.04 (29.50)   72.36 (27.73)   0.256\n  dnr1 = Yes (%)            654 (11.4)      499 (14.1)      155 ( 7.1)    0.228\n  ninsclas (%)                                                            0.194\n     Medicaid               647 (11.3)      454 (12.8)      193 ( 8.8)         \n     Medicare              1458 (25.4)      947 (26.7)      511 (23.4)         \n     Medicare & Medicaid    374 ( 6.5)      251 ( 7.1)      123 ( 5.6)         \n     No insurance           322 ( 5.6)      186 ( 5.2)      136 ( 6.2)         \n     Private               1698 (29.6)      967 (27.2)      731 (33.5)         \n     Private & Medicare    1236 (21.6)      746 (21.0)      490 (22.4)         \n  resp = Yes (%)           2113 (36.8)     1481 (41.7)      632 (28.9)    0.270\n  card = Yes (%)           1931 (33.7)     1007 (28.4)      924 (42.3)    0.295\n  neuro = Yes (%)           693 (12.1)      575 (16.2)      118 ( 5.4)    0.353\n  gastr = Yes (%)           942 (16.4)      522 (14.7)      420 (19.2)    0.121\n  renal = Yes (%)           295 ( 5.1)      147 ( 4.1)      148 ( 6.8)    0.116\n  meta = Yes (%)            265 ( 4.6)      172 ( 4.8)       93 ( 4.3)    0.028\n  hema = Yes (%)            354 ( 6.2)      239 ( 6.7)      115 ( 5.3)    0.062\n  seps = Yes (%)           1031 (18.0)      515 (14.5)      516 (23.6)    0.234\n  trauma = Yes (%)           52 ( 0.9)       18 ( 0.5)       34 ( 1.6)    0.104\n  ortho = Yes (%)             7 ( 0.1)        3 ( 0.1)        4 ( 0.2)    0.027\n  race (%)                                                                0.036\n     black                  920 (16.0)      585 (16.5)      335 (15.3)         \n     other                  355 ( 6.2)      213 ( 6.0)      142 ( 6.5)         \n     white                 4460 (77.8)     2753 (77.5)     1707 (78.2)         \n  income (%)                                                              0.142\n     $11-$25k              1165 (20.3)      713 (20.1)      452 (20.7)         \n     $25-$50k               893 (15.6)      500 (14.1)      393 (18.0)         \n     &gt; $50k                 451 ( 7.9)      257 ( 7.2)      194 ( 8.9)         \n     Under $11k            3226 (56.3)     2081 (58.6)     1145 (52.4)"
  },
  {
    "objectID": "section1.html#naive-regression-analysis",
    "href": "section1.html#naive-regression-analysis",
    "title": "1. The implementation of standard causal inference aproaches in R",
    "section": "0.1 Naive regression analysis",
    "text": "0.1 Naive regression analysis\n\nlibrary(sjPlot)\n# adjust the treatment variable & baseline variables;\nfit0.Y.formula &lt;- as.formula(paste(\"Y ~ A +\", \n                               paste(baselines, \n                                     collapse = \"+\")))\nfit0 &lt;- glm(fit0.Y.formula, family = \"binomial\", data = data2)\ntab_model(fit0)\n\n\n\n\n\n\n\n\n\n\n \nY\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n1.84\n0.00 – 681.93\n0.841\n\n\nA\n1.42\n1.23 – 1.64\n&lt;0.001\n\n\ncat1 [CHF]\n0.62\n0.43 – 0.89\n0.010\n\n\ncat1 [Cirrhosis]\n1.28\n0.82 – 1.98\n0.271\n\n\ncat1 [Colon Cancer]\n0.08\n0.00 – 0.78\n0.048\n\n\ncat1 [Coma]\n3.90\n2.92 – 5.25\n&lt;0.001\n\n\ncat1 [COPD]\n0.59\n0.42 – 0.83\n0.002\n\n\ncat1 [Lung Cancer]\n2.04\n0.94 – 4.31\n0.064\n\n\ncat1 [MOSF w/Malignancy]\n1.67\n1.25 – 2.24\n0.001\n\n\ncat1 [MOSF w/Sepsis]\n1.02\n0.85 – 1.22\n0.819\n\n\nca [No]\n0.40\n0.18 – 0.87\n0.021\n\n\nca [Yes]\n0.86\n0.64 – 1.14\n0.289\n\n\ncardiohx\n1.02\n0.84 – 1.23\n0.857\n\n\nchfhx\n1.06\n0.86 – 1.29\n0.586\n\n\ndementhx\n1.11\n0.90 – 1.37\n0.330\n\n\npsychhx\n0.79\n0.61 – 1.03\n0.092\n\n\nchrpulhx\n1.07\n0.88 – 1.29\n0.523\n\n\nrenalhx\n1.28\n0.89 – 1.82\n0.180\n\n\nliverhx\n1.34\n0.93 – 1.93\n0.110\n\n\ngibledhx\n1.22\n0.80 – 1.87\n0.357\n\n\nmalighx\n0.61\n0.30 – 1.23\n0.165\n\n\nimmunhx\n1.08\n0.94 – 1.25\n0.290\n\n\ntranshx\n1.17\n0.96 – 1.43\n0.112\n\n\namihx\n0.74\n0.52 – 1.05\n0.099\n\n\nage\n1.01\n1.01 – 1.02\n&lt;0.001\n\n\nsex [Male]\n1.10\n0.96 – 1.25\n0.167\n\n\nedu\n1.00\n0.98 – 1.02\n0.854\n\n\naps1\n1.02\n1.01 – 1.02\n&lt;0.001\n\n\nscoma1\n1.01\n1.01 – 1.01\n&lt;0.001\n\n\nmeanbp1\n1.00\n1.00 – 1.00\n0.120\n\n\nwblc1\n1.00\n0.99 – 1.00\n0.448\n\n\nhrt1\n1.00\n1.00 – 1.00\n0.042\n\n\nresp1\n1.00\n0.99 – 1.00\n0.064\n\n\ntemp1\n0.97\n0.93 – 1.01\n0.107\n\n\npafi1\n1.00\n1.00 – 1.00\n0.004\n\n\nalb1\n0.96\n0.87 – 1.05\n0.395\n\n\nhema1\n1.00\n0.99 – 1.01\n0.915\n\n\nbili1\n1.06\n1.05 – 1.08\n&lt;0.001\n\n\ncrea1\n0.97\n0.93 – 1.02\n0.207\n\n\nsod1\n0.99\n0.98 – 1.00\n0.071\n\n\npot1\n1.00\n0.94 – 1.07\n0.964\n\n\npaco21\n1.00\n0.99 – 1.00\n0.303\n\n\nph1\n0.90\n0.43 – 1.88\n0.770\n\n\nwtkilo1\n1.00\n1.00 – 1.00\n0.444\n\n\ndnr1 [Yes]\n3.00\n2.47 – 3.65\n&lt;0.001\n\n\nninsclas [Medicare]\n1.42\n1.09 – 1.85\n0.010\n\n\nninsclas [Medicare &\nMedicaid]\n1.20\n0.87 – 1.67\n0.270\n\n\nninsclas [No insurance]\n1.66\n1.19 – 2.31\n0.003\n\n\nninsclas [Private]\n1.28\n1.00 – 1.64\n0.053\n\n\nninsclas [Private &\nMedicare]\n1.26\n0.96 – 1.65\n0.103\n\n\nresp [Yes]\n1.19\n1.01 – 1.40\n0.037\n\n\ncard [Yes]\n0.98\n0.83 – 1.15\n0.780\n\n\nneuro [Yes]\n1.33\n1.05 – 1.69\n0.017\n\n\ngastr [Yes]\n1.13\n0.92 – 1.39\n0.233\n\n\nrenal [Yes]\n0.96\n0.72 – 1.29\n0.800\n\n\nmeta [Yes]\n0.78\n0.57 – 1.06\n0.116\n\n\nhema [Yes]\n1.91\n1.46 – 2.49\n&lt;0.001\n\n\nseps [Yes]\n0.98\n0.82 – 1.18\n0.862\n\n\ntrauma [Yes]\n0.94\n0.45 – 1.85\n0.866\n\n\northo [Yes]\n0.45\n0.02 – 2.87\n0.470\n\n\nrace [other]\n1.08\n0.80 – 1.46\n0.610\n\n\nrace [white]\n1.04\n0.86 – 1.25\n0.712\n\n\nincome [$25-$50k]\n0.83\n0.66 – 1.03\n0.089\n\n\nincome [&gt; $50k]\n0.82\n0.62 – 1.08\n0.166\n\n\nincome [Under $11k]\n1.17\n0.99 – 1.39\n0.071\n\n\nObservations\n5735\n\n\nR2 Tjur\n0.205"
  },
  {
    "objectID": "section1.html#propensity-score-analysis",
    "href": "section1.html#propensity-score-analysis",
    "title": "1. The implementation of standard causal inference aproaches in R",
    "section": "1 Propensity score analysis",
    "text": "1 Propensity score analysis\n\nThis is a review from the morning session. See Brice’s lecture notes for details.\n\n\n1.1 Propensity score matching\n\nlibrary(MatchIt)\n\nset.seed(123) #this approach requires setting seed values to reproduce the same results;\n\nps.formula &lt;- as.formula(paste(\"A~\", \n                paste(baselines, collapse = \"+\")))\n\nPS.fit &lt;- glm(ps.formula,family=\"binomial\", data=data2)\n\n#adding calculated PS values back to the dataset;\ndata2$PS &lt;- predict(PS.fit, newdata = data2, type=\"response\") \n\n# we can select variables caliper to ensure large/representable matched pairs are selected;\n# caliper can take values between 0.05 (a tight match) to 0.2 (a loose match);\n\nmatch.obj &lt;- matchit(ps.formula, data =data2,\n                     distance = data2$PS,\n                     method = \"nearest\", #nearest neighbour;\n                     replace=FALSE,\n                     ratio = 1, #1:1 match;\n                     caliper = .15)\n\nChecking PS distributions and balance\n\nlibrary(cobalt)\nbal.plot(match.obj,\n         var.name=\"distance\",\n         which=\"both\",\n         type = \"density\",\n         colors = c(\"red\",\"blue\"))\n\n\n\n\n\n\n\n\n\nlove.plot(match.obj, \n          binary = \"std\", \n          grid = TRUE,\n          thresholds = c(m = .1),\n          colors = c(\"red\",\"blue\"))  \n\n\n\n\n\n\n\n\nOutcome regression post-matching\n\nlibrary(geepack)\nMatch &lt;- match.data(match.obj)\nfit1 &lt;- geeglm(Y ~ A, family=binomial(\"log\"), \n              data=Match,\n              weights=weights, \n              std.err = 'san.se', \n              id=subclass, \n              corstr=\"independence\") \nsjPlot::tab_model(fit1)\n\n\n\n\n \nY\n\n\nPredictors\nRisk Ratios\nCI\np\n\n\n(Intercept)\n0.30\n0.28 – 0.33\n&lt;0.001\n\n\nA\n1.23\n1.12 – 1.36\n&lt;0.001\n\n\n\nN subclass\n1635\n\nObservations\n3270\n\n\n\n\n\n\n\n\n1.2 Propensity score weighting\nObtaining weights using WeightIt package\n\nlibrary(WeightIt)\nIPTW &lt;- weightit(ps.formula,\n                 data = data2,\n                 method = \"glm\", #using the default logistic regression;\n                 stabilize = TRUE)\n\nIPTW\n\nA weightit object\n - method: \"glm\" (propensity score weighting with GLM)\n - number of obs.: 5735\n - sampling weights: none\n - treatment: 2-category\n - estimand: ATE\n - covariates: cat1, ca, cardiohx, chfhx, dementhx, psychhx, chrpulhx, renalhx, liverhx, gibledhx, malighx, immunhx, transhx, amihx, age, sex, edu, aps1, scoma1, meanbp1, wblc1, hrt1, resp1, temp1, pafi1, alb1, hema1, bili1, crea1, sod1, pot1, paco21, ph1, wtkilo1, dnr1, ninsclas, resp, card, neuro, gastr, renal, meta, hema, seps, trauma, ortho, race, income\n\nsummary(IPTW)\n\n                 Summary of weights\n\n- Weight ranges:\n\n           Min                                   Max\ntreated 0.3893 |---------------------------| 19.6399\ncontrol 0.6206 |--------------|              11.4347\n\n- Units with the 5 most extreme weights by group:\n                                              \n            795   4885    1825    3828    4890\n treated 7.5334 7.9065  8.0164  8.5311 19.6399\n           2174   4007     505    3216    2986\n control 8.8226  9.686 10.0262 10.1506 11.4347\n\n- Weight statistics:\n\n        Coef of Var   MAD Entropy # Zeros\ntreated       0.959 0.522   0.258       0\ncontrol       0.622 0.339   0.117       0\n\n- Effective Sample Sizes:\n\n           Control Treated\nUnweighted 3551.   2184.  \nWeighted   2561.57 1137.96\n\n\nChecking PS distributions and balance\n\nbal.plot(IPTW,\n         which=\"both\",\n         type = \"density\",\n         colors = c(\"red\",\"blue\"))\n\n\n\n\n\n\n\nbal.tab(IPTW, un=TRUE, thresholds = c(m=0.1))\n\nBalance Measures\n                                 Type Diff.Un Diff.Adj    M.Threshold\nprop.score                   Distance  1.1831   0.0353 Balanced, &lt;0.1\ncat1_ARF                       Binary -0.0290   0.0099 Balanced, &lt;0.1\ncat1_CHF                       Binary  0.0261   0.0013 Balanced, &lt;0.1\ncat1_Cirrhosis                 Binary -0.0268  -0.0025 Balanced, &lt;0.1\ncat1_Colon Cancer              Binary -0.0012  -0.0004 Balanced, &lt;0.1\ncat1_Coma                      Binary -0.0525   0.0007 Balanced, &lt;0.1\ncat1_COPD                      Binary -0.0858  -0.0135 Balanced, &lt;0.1\ncat1_Lung Cancer               Binary -0.0073  -0.0033 Balanced, &lt;0.1\ncat1_MOSF w/Malignancy         Binary  0.0045   0.0023 Balanced, &lt;0.1\ncat1_MOSF w/Sepsis             Binary  0.1721   0.0054 Balanced, &lt;0.1\nca_Metastatic                  Binary -0.0172  -0.0021 Balanced, &lt;0.1\nca_No                          Binary  0.0439   0.0005 Balanced, &lt;0.1\nca_Yes                         Binary -0.0267   0.0017 Balanced, &lt;0.1\ncardiohx                       Binary  0.0445   0.0048 Balanced, &lt;0.1\nchfhx                          Binary  0.0268   0.0000 Balanced, &lt;0.1\ndementhx                       Binary -0.0472  -0.0127 Balanced, &lt;0.1\npsychhx                        Binary -0.0348  -0.0033 Balanced, &lt;0.1\nchrpulhx                       Binary -0.0737  -0.0066 Balanced, &lt;0.1\nrenalhx                        Binary  0.0066   0.0023 Balanced, &lt;0.1\nliverhx                        Binary -0.0124   0.0005 Balanced, &lt;0.1\ngibledhx                       Binary -0.0122  -0.0026 Balanced, &lt;0.1\nmalighx                        Binary -0.0423  -0.0004 Balanced, &lt;0.1\nimmunhx                        Binary  0.0358  -0.0019 Balanced, &lt;0.1\ntranshx                        Binary  0.0554   0.0053 Balanced, &lt;0.1\namihx                          Binary  0.0139   0.0011 Balanced, &lt;0.1\nage                           Contin. -0.0614  -0.0015 Balanced, &lt;0.1\nsex_Male                       Binary  0.0462   0.0142 Balanced, &lt;0.1\nedu                           Contin.  0.0914   0.0094 Balanced, &lt;0.1\naps1                          Contin.  0.5014   0.0240 Balanced, &lt;0.1\nscoma1                        Contin. -0.1098  -0.0116 Balanced, &lt;0.1\nmeanbp1                       Contin. -0.4551  -0.0030 Balanced, &lt;0.1\nwblc1                         Contin.  0.0836   0.0498 Balanced, &lt;0.1\nhrt1                          Contin.  0.1469   0.0305 Balanced, &lt;0.1\nresp1                         Contin. -0.1655   0.0120 Balanced, &lt;0.1\ntemp1                         Contin. -0.0214   0.0030 Balanced, &lt;0.1\npafi1                         Contin. -0.4332  -0.0017 Balanced, &lt;0.1\nalb1                          Contin. -0.2299  -0.0189 Balanced, &lt;0.1\nhema1                         Contin. -0.2693  -0.0321 Balanced, &lt;0.1\nbili1                         Contin.  0.1446  -0.0080 Balanced, &lt;0.1\ncrea1                         Contin.  0.2696   0.0198 Balanced, &lt;0.1\nsod1                          Contin. -0.0922  -0.0124 Balanced, &lt;0.1\npot1                          Contin. -0.0271  -0.0259 Balanced, &lt;0.1\npaco21                        Contin. -0.2486  -0.0276 Balanced, &lt;0.1\nph1                           Contin. -0.1198   0.0167 Balanced, &lt;0.1\nwtkilo1                       Contin.  0.2557   0.0202 Balanced, &lt;0.1\ndnr1_Yes                       Binary -0.0696  -0.0105 Balanced, &lt;0.1\nninsclas_Medicaid              Binary -0.0395   0.0046 Balanced, &lt;0.1\nninsclas_Medicare              Binary -0.0327  -0.0136 Balanced, &lt;0.1\nninsclas_Medicare & Medicaid   Binary -0.0144   0.0009 Balanced, &lt;0.1\nninsclas_No insurance          Binary  0.0099   0.0002 Balanced, &lt;0.1\nninsclas_Private               Binary  0.0624   0.0041 Balanced, &lt;0.1\nninsclas_Private & Medicare    Binary  0.0143   0.0037 Balanced, &lt;0.1\nresp_Yes                       Binary -0.1277  -0.0096 Balanced, &lt;0.1\ncard_Yes                       Binary  0.1395   0.0084 Balanced, &lt;0.1\nneuro_Yes                      Binary -0.1079  -0.0077 Balanced, &lt;0.1\ngastr_Yes                      Binary  0.0453   0.0002 Balanced, &lt;0.1\nrenal_Yes                      Binary  0.0264   0.0049 Balanced, &lt;0.1\nmeta_Yes                       Binary -0.0059   0.0010 Balanced, &lt;0.1\nhema_Yes                       Binary -0.0146   0.0013 Balanced, &lt;0.1\nseps_Yes                       Binary  0.0912   0.0054 Balanced, &lt;0.1\ntrauma_Yes                     Binary  0.0105   0.0011 Balanced, &lt;0.1\northo_Yes                      Binary  0.0010   0.0002 Balanced, &lt;0.1\nrace_black                     Binary -0.0114   0.0057 Balanced, &lt;0.1\nrace_other                     Binary  0.0050  -0.0026 Balanced, &lt;0.1\nrace_white                     Binary  0.0063  -0.0032 Balanced, &lt;0.1\nincome_$11-$25k                Binary  0.0062  -0.0081 Balanced, &lt;0.1\nincome_$25-$50k                Binary  0.0391   0.0051 Balanced, &lt;0.1\nincome_&gt; $50k                  Binary  0.0165   0.0013 Balanced, &lt;0.1\nincome_Under $11k              Binary -0.0618   0.0017 Balanced, &lt;0.1\n\nBalance tally for mean differences\n                   count\nBalanced, &lt;0.1        69\nNot Balanced, &gt;0.1     0\n\nVariable with the greatest mean difference\n Variable Diff.Adj    M.Threshold\n    wblc1   0.0498 Balanced, &lt;0.1\n\nEffective sample sizes\n           Control Treated\nUnadjusted 3551.   2184.  \nAdjusted   2561.57 1137.96\n\n\n\nlove.plot(IPTW, \n          binary = \"std\", \n          grid = TRUE,\n          thresholds = c(m = .1),\n          colors = c(\"red\",\"blue\"))  \n\n\n\n\n\n\n\n\nOutcome modelling\n\nfit2 &lt;- glm(Y ~ A, \n            family = \"binomial\",\n            weights = IPTW$weights,\n            data = data2)\n\n#trim weights;\n#Trimming at 99th percentile\nIPTW.trim &lt;- trim(IPTW, at = .99)\n\nfit2.trim &lt;- glm(Y ~ A, \n            family = \"binomial\",\n            weights = IPTW.trim$weights,\n            data = data2)\ntab_model(fit2, fit2.trim)\n\n\n\n\n \nY\nY\n\n\nPredictors\nOdds Ratios\nCI\np\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.45\n0.42 – 0.48\n&lt;0.001\n0.45\n0.42 – 0.48\n&lt;0.001\n\n\nA\n1.30\n1.16 – 1.45\n&lt;0.001\n1.34\n1.19 – 1.50\n&lt;0.001\n\n\nObservations\n5735\n5735\n\n\nR2 Tjur\n0.005\n0.005"
  },
  {
    "objectID": "section1.html#general-guildelines-on-implementing-psa",
    "href": "section1.html#general-guildelines-on-implementing-psa",
    "title": "1. The implementation of standard causal inference aproaches in R",
    "section": "1.3 General guildelines on implementing PSA",
    "text": "1.3 General guildelines on implementing PSA\nHere providing a list a references that covers guildelines are PSA analysis and its reporting\n\nPS matching (Austin 2007)\nPSA analysis for cancer research (Yao et al. 2017)\nPSA analysis for neurology (Austin et al. 2021)\nPSA analysis for multiple sclerosis (Karim et al. 2022)\n\n\nChecklist for the design and analysis of studies using PS methods\n(Austin et al. 2021)\n\n\n\n\n\n\n\nChecklist for reporting of studies using PS methods\n(Austin et al. 2021)\n\n\n\n\n\nSubgroup analysis - There are several good resources on how to estimate subgroup treatment effects:(Eeren et al. 2015), (Wang et al. 2018), (Dong et al. 2020),(Yang et al. 2021)\n\nThe Subgroup Balancing Propensity Score approach from (Dong et al. 2020) has been incorporated in the WeightIt package.\n\n\n# example subgroup analysis by sex;\n(fit2&lt;- weightit(treat ~ age + educ + married +\n                nodegree + race + re74, data = lalonde,\n                method = \"glm\", estimand = \"ATT\"))\n\nA weightit object\n - method: \"glm\" (propensity score weighting with GLM)\n - number of obs.: 614\n - sampling weights: none\n - treatment: 2-category\n - estimand: ATT (focal: 1)\n - covariates: age, educ, married, nodegree, race, re74\n\nIPTW_sex &lt;- weightit(ps.formula,\n                 data = data2,\n                 method = \"glm\", #using the default logistic regression;\n                 by = \"sex\",\n                 stabilize = TRUE)\n\nsubgroup_sex &lt;- sbps(IPTW, IPTW_sex)\nsummary(subgroup_sex)\n\nSummary of weights:\n\n - Overall vs. subgroup proportion contribution:\n         sex = Male sex = Female\nOverall           0            0\nSubgroup          1            1\n\n - - - - - - - Subgroup sex = Male - - - - - - -\n- Weight ranges:\n           Min                                   Max\ntreated 0.4004 |---------------------------| 26.2701\ncontrol 0.5996  |-------|                     8.8109\n\n- Units with 5 greatest weights by group:\n                                            \n           2719   2716   2474   1957    1064\n treated 6.4907 7.3707 8.2499 9.2112 26.2701\n           2224   1672   1209    821     283\n control 6.3468 6.6169 7.8151  8.621  8.8109\n\n          Ratio Coef of Var\ntreated 65.6135      1.1156\ncontrol 14.6941      0.6237\noverall 65.6135      0.8532\n\n- Effective Sample Sizes:\n            Control  Treated\nUnweighted 1914.000 1278.000\nWeighted   1378.187  569.595\n\n - - - - - - - Subgroup sex = Female - - - - - - -\n- Weight ranges:\n           Min                                   Max\ntreated 0.3626 |--------------------|         9.2546\ncontrol 0.6444  |--------------------------| 12.1700\n\n- Units with 5 greatest weights by group:\n                                           \n           2177   1704   1626    806    765\n treated 5.8714 6.7055 7.3113 7.7407 9.2546\n           1498   1415   1378    611    432\n control 5.3371 7.2264 7.5463 7.6013  12.17\n\n          Ratio Coef of Var\ntreated 25.5239      0.9529\ncontrol 18.8869      0.6245\noverall 33.5645      0.7508\n\n- Effective Sample Sizes:\n           Control Treated\nUnweighted 1637.00  906.00\nWeighted   1177.92  475.07\n\n\nChecking PS distribution by subgroups\n\nbal.plot(IPTW_sex, \n         cluster = \"sex\",\n         which=\"both\",\n         type = \"density\",\n         colors = c(\"red\",\"blue\"))\n\n\n\n\n\n\n\n\nchecking balance by subgroups\n\n# checking balance by subgroups;\n# bal.tab(IPTW_sex, cluster = \"sex\")\nlove.plot(IPTW_sex, \n          cluster = \"sex\", \n          grid = TRUE,\n          stars = \"std\",\n          thresholds = c(m = .1),\n          colors = c(\"red\",\"blue\"))\n\n\n\n\n\n\n\n\nfitting subgroup specific causal outcome models\n\nfit2.f &lt;- glm(Y ~ A, \n            family = \"binomial\",\n            weights = IPTW_sex$weights[IPTW_sex$by==\"Female\"],\n            data = subset(data2,sex==\"Female\"))\nfit2.m &lt;- glm(Y ~ A, \n            family = \"binomial\",\n            weights = IPTW_sex$weights[IPTW_sex$by==\"Male\"],\n            data = subset(data2,sex==\"Male\"))\ntab_model(fit2.f, fit2.m)\n\n\n\n\n \nY\nY\n\n\nPredictors\nOdds Ratios\nCI\np\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.41\n0.37 – 0.46\n&lt;0.001\n0.49\n0.44 – 0.54\n&lt;0.001\n\n\nA\n1.48\n1.24 – 1.76\n&lt;0.001\n1.18\n1.02 – 1.37\n0.029\n\n\nObservations\n2543\n3192\n\n\nR2 Tjur\n0.007\n0.003\n\n\n\n\n\n\n\nSensitivity analysis for unmeasured confounding\n\nE-value, see online calculator https://www.evalue-calculator.com/\nYou can calculate e-value in R as well using package Evalue, see https://louisahsmith.github.io/evalue/index.html"
  },
  {
    "objectID": "section2.html",
    "href": "section2.html",
    "title": "2. Using machine learning techniques for causal analysis",
    "section": "",
    "text": "Outlines\n\n\n\n\nPropensity score methods machine learning techniques\n\ngradient boosting machines\nsuper learner\nBayesian additive regression trees"
  },
  {
    "objectID": "section2.html#super-machine-learning",
    "href": "section2.html#super-machine-learning",
    "title": "2. Using machine learning techniques for causal analysis",
    "section": "2.1 Super (Machine) Learning",
    "text": "2.1 Super (Machine) Learning\n\nSuper learning can be used to obtain robust estimator. In a nut-shell it uses loss-function based ML tool and cross-validation to obtain the best prediction of model parameter of interest, based on a weighted average of a library of machine learning algorithms.\nGuide to SuperLearner by Chris Kennedy at https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html\nNew visual guide created by Katherine Hoffman\n\n\n\n\n\n\n\nList of machine learning algorithms under SuperLearner R package\n\n\nlibrary(SuperLearner)\nlistWrappers()\n\n [1] \"SL.bartMachine\"      \"SL.bayesglm\"         \"SL.biglasso\"        \n [4] \"SL.caret\"            \"SL.caret.rpart\"      \"SL.cforest\"         \n [7] \"SL.earth\"            \"SL.extraTrees\"       \"SL.gam\"             \n[10] \"SL.gbm\"              \"SL.glm\"              \"SL.glm.interaction\" \n[13] \"SL.glmnet\"           \"SL.ipredbagg\"        \"SL.kernelKnn\"       \n[16] \"SL.knn\"              \"SL.ksvm\"             \"SL.lda\"             \n[19] \"SL.leekasso\"         \"SL.lm\"               \"SL.loess\"           \n[22] \"SL.logreg\"           \"SL.mean\"             \"SL.nnet\"            \n[25] \"SL.nnls\"             \"SL.polymars\"         \"SL.qda\"             \n[28] \"SL.randomForest\"     \"SL.ranger\"           \"SL.ridge\"           \n[31] \"SL.rpart\"            \"SL.rpartPrune\"       \"SL.speedglm\"        \n[34] \"SL.speedlm\"          \"SL.step\"             \"SL.step.forward\"    \n[37] \"SL.step.interaction\" \"SL.stepAIC\"          \"SL.svm\"             \n[40] \"SL.template\"         \"SL.xgboost\"         \n[1] \"All\"\n[1] \"screen.corP\"           \"screen.corRank\"        \"screen.glmnet\"        \n[4] \"screen.randomForest\"   \"screen.SIS\"            \"screen.template\"      \n[7] \"screen.ttest\"          \"write.screen.template\""
  },
  {
    "objectID": "section2.html#using-machine-learning-methods-with-psa",
    "href": "section2.html#using-machine-learning-methods-with-psa",
    "title": "2. Using machine learning techniques for causal analysis",
    "section": "2.2 Using machine learning methods with PSA",
    "text": "2.2 Using machine learning methods with PSA\n\nWe can use ML to model our propensity score model\nThe use of machine learning methods is more flexible than parametric methods (i.e., logistic regression)\n\nNot without a cost, usually the more flexible the methods are the more one is at risk of overfitting; Too much noise considered in the modelling often results in poor coverage probability.\nThere is no one approach that out performs others, thus which approach to use should be evaluated case by case.\nML is generally suggested for large enough cohort and for modelling large set of covariates.\nIt’s always suggested to include results from conventional logistic regression approach as a sensitivity analysis in comparison of ML approaches.\n\nMany approaches are included in the WeightIt package, https://ngreifer.github.io/WeightIt/reference/method_super.html\n\n“gbm”, Propensity score weighting using generalized boosted modeling (also known as gradient boosting machines)\n“super”, Propensity score weighting using SuperLearner\n“bart”, Propensity score weighting using Bayesian additive regression trees (BART)\n\nBayesian Additive Regression Trees (BART) is a sum-of-trees model for approximating an unknown function. To avoid overfitting (of decision tree), BART uses a regularization prior that forces each tree to be able to explain only a limited subset of the relationships between the covariates and the predictor variable.\n\n\n\nSetting up data and PS model formula\n\nrequire(tidyverse)\nrequire(WeightIt)\ndata2&lt;-readRDS(\"data/data2\")\ncovariates &lt;- select(data2, -c(id, A, Y))\nbaselines &lt;- colnames(covariates)\n\nps.formula &lt;- as.formula(paste(\"A~\", \n                paste(baselines, collapse = \"+\")))\n\n\n2.2.1 PS model with gradient boosting\n\ncomputationally more demanding and it might take several minutes to run.\n\n\nIPTW_gbm &lt;- weightit(ps.formula,\n                 data = data2,\n                 method = \"gbm\",\n                 stabilize = TRUE)\n# saving the model output as a R object to avoid rerunning the same model;\nsaveRDS(IPTW_gbm, file = \"data/IPTW_gbm\")\n\n\n# reading saved model output;\nrequire(sjPlot)\nIPTW_gbm &lt;- readRDS(file = \"data/IPTW_gbm\")\nsummary(IPTW_gbm)\n\n                 Summary of weights\n\n- Weight ranges:\n\n           Min                                   Max\ntreated 0.3874 |---------------------------| 10.9875\ncontrol 0.6227  |------------|                5.6764\n\n- Units with the 5 most extreme weights by group:\n                                            \n           5525   3148   4890   1825    5131\n treated 4.0556 4.6873 5.1931 5.3152 10.9875\n            497   2046   3830   1602    1000\n control 4.6689 4.8977  4.929 4.9313  5.6764\n\n- Weight statistics:\n\n        Coef of Var   MAD Entropy # Zeros\ntreated       0.663 0.397   0.143       0\ncontrol       0.426 0.262   0.067       0\n\n- Effective Sample Sizes:\n\n           Control Treated\nUnweighted 3551.   2184.  \nWeighted   3006.24 1517.47\n\nfit2_gbm &lt;- glm(Y ~ A, \n            family = \"binomial\",\n            weights = IPTW_gbm$weights,\n            data = data2)\ntab_model(fit2_gbm)\n\n\n\n\n \nY\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.46\n0.42 – 0.49\n&lt;0.001\n\n\nA\n1.27\n1.12 – 1.43\n&lt;0.001\n\n\nObservations\n5735\n\n\nR2 Tjur\n0.004\n\n\n\n\n\n\n\n\n\n2.2.2 PS model with Super Learner\n\nIPTW_SL &lt;- weightit(ps.formula,\n                 data = data2,\n                 method = \"super\",\n                 SL.library=c(\"SL.randomForest\", \"SL.glmnet\", \"SL.nnet\"), \n                 stabilize = TRUE)\n# saving the model output as a R object to avoid rerunning the same model;\nsaveRDS(IPTW_SL, file = \"data/IPTW_SL\")\n\n\n# reading saved model output;\nIPTW_SL &lt;- readRDS(file = \"data/IPTW_SL\")\nsummary(IPTW_SL)\n\n                 Summary of weights\n\n- Weight ranges:\n\n           Min                                  Max\ntreated 0.4008 |-----------------|           1.0346\ncontrol 0.6254        |--------------------| 1.4140\n\n- Units with the 5 most extreme weights by group:\n                                           \n            742   5131   3508   1825   4890\n treated 0.9223 0.9489  0.968 0.9808 1.0346\n           3391   3216   2174   2986    505\n control 1.3373 1.3461 1.3533 1.3627  1.414\n\n- Weight statistics:\n\n        Coef of Var   MAD Entropy # Zeros\ntreated       0.184 0.147   0.016       0\ncontrol       0.159 0.125   0.012       0\n\n- Effective Sample Sizes:\n\n           Control Treated\nUnweighted 3551.    2184. \nWeighted   3463.76  2112.6\n\nfit2_SL &lt;- glm(Y ~ A, \n            family = \"binomial\",\n            weights = IPTW_SL$weights,\n            data = data2)\ntab_model(fit2_SL)\n\n\n\n\n \nY\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.45\n0.41 – 0.48\n&lt;0.001\n\n\nA\n1.35\n1.17 – 1.56\n&lt;0.001\n\n\nObservations\n5735\n\n\nR2 Tjur\n0.005\n\n\n\n\n\n\n\n\n\n2.2.3 PS model with Bayesian additive regression trees\n\nA much faster algorithm comparing to gbm and SL.\n\n\nIPTW_bart &lt;- weightit(ps.formula,\n                 data = data2,\n                 method = \"bart\",\n                 stabilize = TRUE)\n# saving the model output as a R object to avoid rerunning the same model;\nsaveRDS(IPTW_bart, file = \"data/IPTW_bart\")\n\n\n# reading saved model output;\nIPTW_bart &lt;- readRDS(file = \"data/IPTW_bart\")\nsummary(IPTW_bart)\n\n                 Summary of weights\n\n- Weight ranges:\n\n           Min                                   Max\ntreated 0.3861 |---------------------------| 13.8560\ncontrol 0.6211 |------------------|           9.8333\n\n- Units with the 5 most extreme weights by group:\n                                             \n           3148   4119    1825    5131   4890\n treated 8.7361 9.5095 11.8258 12.7766 13.856\n             84   1810    3839    1000    505\n control 6.7109 6.7279  7.9066  7.9218 9.8333\n\n- Weight statistics:\n\n        Coef of Var   MAD Entropy # Zeros\ntreated       0.926 0.471   0.226       0\ncontrol       0.544 0.308   0.096       0\n\n- Effective Sample Sizes:\n\n           Control Treated\nUnweighted 3551.   2184.  \nWeighted   2740.56 1175.59\n\nfit2_bart &lt;- glm(Y ~ A, \n            family = \"binomial\",\n            weights = IPTW_bart$weights,\n            data = data2)\ntab_model(fit2_bart)\n\n\n\n\n \nY\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.45\n0.42 – 0.48\n&lt;0.001\n\n\nA\n1.30\n1.16 – 1.46\n&lt;0.001\n\n\nObservations\n5735\n\n\nR2 Tjur\n0.005\n\n\n\n\n\n\n\n\nComparing the three approaches given the rhc data, it appears that SuperLearner returns good stable weights (no visible extreme weights). This indicates a great fit of the PS model.\n\n\nSimilar as before, we can check for PS distribution and balancing. Additionally, we can perform subgroup and sensitivity analysis as mentioned before.\n\nForest plot to display results from different approaches\n\ncode modified from https://www.khstats.com/blog/forest-plots/ by Katherine Hoffman\n\n\nplotdata &lt;- data.frame(\n  method = c(\"Naive-Reg\",\"PS-match\", \"PS-IPTW\", \"PS-IPTW-Trim\", \"PS-IPTW-gbm\", \"PS-IPTW-SL\", \"PS-IPTW-bart\"),  \n  est.OR = c(1.42, 1.23, 1.30, 1.34, 1.27, 1.35, 1.30),\n  conf.low = c(1.23, 1.12, 1.16, 1.19, 1.12, 1.17, 1.16),\n  conf.high = c(1.64, 1.36, 1.45, 1.50, 1.43, 1.56, 1.46))\n\np &lt;- \n  plotdata |&gt;\n  ggplot(aes(y = fct_rev(method))) + \n  theme_classic() +\n  geom_point(aes(x=est.OR), shape=15, size=3) +\n  geom_linerange(aes(xmin=conf.low, xmax=conf.high)) +\n  geom_vline(xintercept = 1, linetype=\"dashed\") +\n  labs(x=\"Odds Ratio\", y=\"\") +\n  coord_cartesian(ylim=c(1,8), xlim=c(0.7, 2)) +\n  annotate(\"text\", x = 0.8, y = 8, label = \"RHC protective\") +\n  annotate(\"text\", x = 1.2, y = 8, label = \"RHC harmful\") + \n  theme(axis.line.y = element_blank(),\n        axis.ticks.y= element_blank(),\n        axis.text.y= element_blank(),\n        axis.title.y= element_blank())\n\n\nplotdata_OR &lt;- plotdata |&gt;\n  # round estimates and 95% CIs to 2 decimal places for journal specifications\n  mutate(estimate_label = paste0(est.OR, \" (\", conf.low, \"-\", conf.high, \")\"))  |&gt;\n  # add a row of data to be shown on the forest plot as column names;\n  bind_rows(\n    data.frame(\n      method = \"Method\",\n      estimate_label = \"Odds Ratio (95% CI)\"\n    )\n  ) |&gt;\n  mutate(method = fct_rev(fct_relevel(method, \"Method\")))\n\np_left &lt;-\n  plotdata_OR  |&gt;\n  ggplot(aes(y = method)) +\n  geom_text(aes(x = 0, label = method), hjust = 0, fontface = \"bold\")+\n  geom_text(\n    aes(x = 2, label = estimate_label),\n    hjust = 0,\n    fontface = ifelse(plotdata_OR$estimate_label == \"Odds Ratio (95% CI)\", \"bold\", \"plain\")\n  )+\n  theme_void() +\n  coord_cartesian(xlim = c(0, 4))\n\nlibrary(patchwork)\nlayout &lt;- c(\n  area(t = 0, l = 0, b = 15, r = 4), # left plot, starts at the top of the page (0) and goes 15 units down and 4 units to the right;\n  area(t = 1, l = 5, b = 15, r = 9) # middle plot starts a little lower (t=1) because there's no title. starts 1 unit right of the left plot (l=5, whereas left plot is r=4);\n)\n# final plot arrangement\np_left + p +plot_layout(design = layout)"
  }
]